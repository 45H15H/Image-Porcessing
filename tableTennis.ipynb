{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be developing a simple game that is controlled by hand gestures. The game will detect the position of the player's hand in the video frame and use that information to control the direction of the game character."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, make sure to have the following packages installed:\n",
    "* OpenCV\n",
    "* cvzone\n",
    "* PyAutoGUI\n",
    "\n",
    "Install these packages using the following commands:\n",
    "* `pip3 install opencv-python`\n",
    "* `pip3 install cvzone`\n",
    "* `pip3 install pyautogui`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "Import these libraries for use in the project:\n",
    "\n",
    "1. `HandDetector` from `cvzone.HandTrackingModule`: This library is responsible for detecting and tracking hand gestures.\n",
    "\n",
    "2. `cv2` (OpenCV): This library is a computer vision library that is used for image and video processing.\n",
    "\n",
    "3. `pyautogui`: This library provides an interface for automating GUI interactions and is used for controlling the mouse and keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import cv2\n",
    "import pyautogui"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Hand Detector\n",
    "\n",
    "The following cell creates an instance of the `HandDetector` class and assigns it to the variable `detector`. \n",
    "\n",
    "The two arguments passed to the class constructor are:\n",
    "\n",
    "1. `maxHands`: This argument specifies the maximum number of hands that the detector should be able to track. In our case, `maxHands` is set to 1, which means that the detector will only track one hand at a time.\n",
    "\n",
    "2. `detectionConfidence`: This argument sets the minimum confidence level for hand detection. In our case, the `detectionConfidence` is set to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = HandDetector(maxHands=1, detectionCon=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Video Input\n",
    "\n",
    "The following cell creates a `VideoCapture` object and assigns it to the variable `video_capture`. The `VideoCapture` class is part of the OpenCV library and is used to capture video from the camera.\n",
    "\n",
    "The argument passed to the `VideoCapture` constructor, `0`, is the index of the camera that will be used for video capture. In our case, `0` specifies the default camera on the computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "\n",
    "The following cell is for hand gesture detection and control using computer vision and PyAutoGUI library. The script reads frames from a video source and performs hand detection using the `detector.findHands` function. The script then uses the detected hand information to perform actions based on hand gestures. The actions are executed using PyAutoGUI's `pyautogui.press` function. \n",
    "\n",
    "The `cv2.rectangle` function is used to draw rectangles on the processed frame for visual representation. The rectangles are used to indicate the direction of the hand gesture.\n",
    "\n",
    "The script uses a while loop to continuously read frames from the video source. Within the loop, the `fingersUp` function is used to determine the number of fingers that are up. Depending on the number of fingers, the script performs different actions and displays text on the processed frame indicating the direction of the hand gesture.\n",
    "\n",
    "The processed frame is displayed using the `cv2.imshow` function, and the script waits for the user to press 'q' to exit. Upon exit, the script releases the video capture and closes all frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while loop to continuously detect hands\n",
    "# move to mouse to (467, 696) before starting the loop \n",
    "pyautogui.moveTo(467, 696)\n",
    "pyautogui.click()\n",
    "\n",
    "screenWidth, screenHeight = (856, 387)\n",
    "# screenWidth, screenHeight = (856, 250)\n",
    "\n",
    "while True:\n",
    "    # read the video capture\n",
    "    ret, frame = video_capture.read()\n",
    "   \n",
    "    # perform hand detection\n",
    "    hands, img = detector.findHands(frame)\n",
    "\n",
    "    # draw the rectangle on the screen\n",
    "    cv2.rectangle(img, (0, 480), (300, 425), (255, 50, 50), -2) # left rectangle, blue color\n",
    "    cv2.rectangle(img, (640, 480), (400, 425), (50, 50, 255), -2) # right rectangle, red color\n",
    "    \n",
    "    # if there is a hand in the image\n",
    "    if hands: \n",
    "        # get the first hand\n",
    "        hand = hands[0]\n",
    "        # get the landmarks of the hand\n",
    "        lmList = hand[\"lmList\"]\n",
    "        # get the bounding box of the hand\n",
    "        bbox = hand[\"bbox\"]\n",
    "        # get the center of the hand\n",
    "        centerPoint = hand['center'] \n",
    "        # get the type of the hand \n",
    "        handType = hand[\"type\"]\n",
    "        # get the fingers up\n",
    "        fingers = detector.fingersUp(hand)\n",
    "\n",
    "        x = centerPoint[0]\n",
    "        y = centerPoint[1]\n",
    "\n",
    "        # map the hand center to the screen coordinates\n",
    "        screenX, screenY = 900 - (x * screenWidth // frame.shape[1]), (y * screenHeight // frame.shape[0]) + 500\n",
    "        \n",
    "        # palm is visible\n",
    "        if fingers == [1, 1, 1, 1, 1]:\n",
    "            # move the mouse according to the position of the hand, flipped in x axis\n",
    "            pyautogui.moveTo(screenX, screenY, 0.01, pyautogui.easeInBounce)\n",
    "            # pyautogui.moveTo(screenX, screenY)\n",
    "\n",
    "        # if the fist is closed move the mouse to pause button and click it\n",
    "        if fingers == [0, 0, 0, 0, 0]:\n",
    "            pyautogui.moveTo(893, 226)\n",
    "            pyautogui.click()\n",
    "\n",
    "\n",
    "    # show the processed frame \n",
    "    cv2.imshow('frame', img)\n",
    "\n",
    "    # press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the video capture\n",
    "video_capture.release()\n",
    "\n",
    "# close all the frames\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Make sure you have the game window open and focused when running the script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "debc2121511f37620e4e406b8e25acf7be96315b8b48c387eff272a1baf02ae1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
